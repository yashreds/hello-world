{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Applications of Data Science\n# Lab 3-Py\n# Preparing Text Data with Python\n\n## Overview\n\nThis lab introduces you to the basics of text mining and text data preparation. In this lab you will work with a set of 160,000 tweets, which include sentiment labels. \n\nSocial media sentiment is an important indicator of public opinion.  Determining sentiment can be valuable in a number of applications including brand awareness, product launches, and detecting political trends. \n\nRaw text is inherently messy. Machine understanding and analysis is inhibited by the presence of extraneous symbols and words that clutter the text. The exact nature of the required text cleaning depends on the application.  In this case, you will focus on text cleaning to facilitate sentiment classification. The presence of certain words determine the sentiment of the tweet. Words and symbols which are extraneous to this purpose are distractions at best, and a likely source of noise in the analysis. You will follow these steps to prepare the tweet text for analysis: \n\n- Remove punctuation symbols and numerals, leaving only alphabetic characters.\n- Convert all remaining characters to lower case. \n- Remove stopwords like \"the\", \"and\" and \"this\". Since these words are relatively common, yet communicate no particular sentiment, they can bias analytics. \n- Stem all remaining words to their root stem. \n\n\n## What you will need\nTo complete this lab, you will need the following:\n- A web browser and Internet connection\n- An Azure ML workspace\n- The lab files for this lab\n\n\n## Load and transform the tweet data\n\nAs a first step, ensure that you have uploaded the **tweets.csv** and **stopwords.csv** files as new datasets in your Azure Machine Learning workspace. Then use the following code to load the tweets data set and set the column names to convenient values."}, {"source": "%matplotlib inline\nimport pandas as pd\nfrom azureml import Workspace\nws = Workspace()\nds = ws.datasets['tweets.csv']\ndataset = ds.to_dataframe()\ndataset.columns = ['sentiment', 'tweets']\ndataset.head()", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "Examine the head of the data frame shown above, noticing the content of the two columns.\n- The Sentiment column contains a sentiment score {0,4} for negative of positive sentiment of the tweet.\n- The Tweets column contains the actual text of the tweet.\n\n\n## Normalize the text\n\nYou will now normalize the tweet text. The code in the cell below performs the following operations:\n- Remove numbers.\n- Remove punctuation.\n- Convert to lower case."}, {"source": "import string\nsp = string.punctuation\ntweets = dataset['tweets'].tolist()\ntweets = list(map(lambda t: ''.join([\"\" if c.isdigit() else c for c in t]), tweets))\ntweets = list(map(lambda t: ''.join([\"\" if c in sp else c for c in t]), tweets))\ntweets = list(map(str.lower, tweets))\ntweets[:5]", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "Examine the tweet text. All of the characters are lower case and there are no punctuation characters, or numbers. \n\nNext, you will compute the counts and cumulative frequencies of the words in the tweets. The **nltk** package contains two tools to help:\n\nThe ** regexp_tokenize** functuon tokenizes the text. Tokenization is the process of dividing the text into its component tokens. In this case, the tokens are all words, since you are working with normalized text.\n\nThe ** FreqDist** function computes the frequency distribution of words in a text corpus. A Pandas data frame is then computed from the word frequency array. \n\nExecute the code in the cell below to compute the word frequency and examine the head of the data frame.\n"}, {"source": "def to_TF(tweets):\n    import pandas as pd\n    import nltk\n    tweets = list(map(lambda t: nltk.regexp_tokenize(t, r'\\S+'), tweets))\n    tweets = [w for l in tweets for w in l]\n\n    ## Compute the frequency distribution of the words as a dictionary\n    fdist = nltk.FreqDist(tweets) \n    ## Convert the dictionary to a dataframe contaning the words and\n    ## counts indexed by the words, and then take the transpose.\n    count_frame = pd.DataFrame(fdist, index =[0]).T\n    count_frame.columns = ['Count']\n    return(count_frame.sort_values('Count', ascending = False))\n \nwf = to_TF(tweets)\nwf.head(n = 20)", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "Notice that the most frequent words are in the head of this data frame. Of these 20 most frequent words none are likely to convey much information on sentiment.  \n\nThe code in the cell below, creates a bar plot of word frequency for the most common 60 words. "}, {"source": "def wf_bar(wf):\n    import matplotlib.pyplot as plt\n    ## Barplot of the most fequent words.   \n    fig = plt.figure(figsize=(12, 9))\n    ax = fig.gca()    \n    wf['Count'][:60].plot(kind = 'bar', ax = ax)\n    ax.set_title('Frequency of the most common words')\n    ax.set_ylabel('Frequency of word')\n    ax.set_xlabel('Word')\n    plt.show()\n    return 'Done'\nwf_bar(wf)", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "Examine this plot and notice the most frequent words. Many of the most frequent words are stop words, such as \"\u00c3\u0082\u00c2\u0098the\", \"and\", and \"you\", which are not likely to be helpful in determining sentiment. Also, the frequency of the words drops off fairly quickly to less than 500 out of the 160,000 tweets.\n\nAnother tool for examining the frequency of words in a corpus of documents is the cumulative distribution frequency (CDF) plot. Execute the code in the cell below to compute and display a bar plot of the cumulative frequencies of the words."}, {"source": "def plot_cfd(wf):\n    import matplotlib.pyplot as plt\n    ## Compute the relative cumulative frequency of the words in \n    ## descending order of frequency and add the dataframe.   \n    word_count = float(wf['Count'].sum(axis = 0))   \n    wf['Cum'] = wf['Count'].cumsum(axis = 0)\n    wf['Cum'] = wf['Cum'].divide(word_count)\n    \n    ## Barplot the cumulative frequency for the most frequent words.   \n    fig = plt.figure(figsize=(12, 9))\n    ax = fig.gca()    \n    wf['Cum'][:60].plot(kind = 'bar', ax = ax)\n    ax.set_title('Cumulative fraction of total words vs. words')\n    ax.set_ylabel('Cumulative fraction')\n    ax.set_xlabel('Word')\n    plt.show()\n    return 'Done'\n\n\nplot_cfd(wf)", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "The conclusions one can draw from the second chart are largely the same as the first. The most frequent words are stop words and the frequency of words drops off rather quickly. Also notice, that the frequency of the words becomes uniform fairly quickly. \n\n****\n\nYou will now examine the head of the resulting word frequency data frame to determine the following:\n- What is the percentage of all words for these first 20 words?\n- Of these 20 words, how many are likely to contibute sentiment information?\n- Are these 20 words different from the words seen for the raw text?\n\nTo perform this exercise, apply the Pandas **head** method, with **n = 20**, to the wf data frame.\n\n"}, {"source": "wf.head(n = 20)", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove stop words\n\nIn the previous section you removed extraneous characters and whitespace from the tweet text. The results show that the most frequent words do not communicate much sentiment information. These frequent words, which are largely extraneous, are known as stop words and should be removed from the text before further analysis. In this exercise you will use custom Python code to remove stop words from the tweet text.\n\nAs a first step you will load the list of stop words, and examine the first 100 by executing the code in the cell below. "}, {"source": "sw = ws.datasets['stopwords.csv']\nstop_words = sw.to_dataframe()\nstop_words = [w for w in stop_words.words if w in stop_words.words.unique() ]\nstop_words[:20]", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "Execute the code in the cell below to remove the stop words from each tweet using nested list comprehensions. "}, {"source": "temp = [tweet.split() for tweet in tweets] ## Split tweets into tokens\ntweets = [' '.join([word for word in tweet if word not in set(stop_words)]) for tweet in temp]", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "Execute the code in the cell below to visualize the word frequency."}, {"source": "wf = to_TF(tweets)\nwf_bar(wf)", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "The distribution of word frequency is not quite different. Note that many of the most frequent words are now likely to convey some sentiment, such as \"good\", \"like\", and \"love\". Evidently, removing stop words has had the desired effect.\n\nNext, execute the code in the cell below to display the CDF of the tweets with the stop words removed. "}, {"source": "plot_cfd(wf)", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "As before, this chart shows a number of frequent words which are likely to convey sentiment. However, note that these 60 most frequent words only make up about 17% or the total.\n\n****\n\nYou will now examine the head of the resulting word frequency data frame to determine the following:\n\n- What is the percentage of all words for these first 20 words? \n- Of these 20 words, how many are likely to contribute sentiment information? \n- Are these 20 words different from the words seen for the normalized text? \n\nTo perform this exercise, apply the Pandas **head** method, with **n = 20**, to the wf data frame.\n****"}, {"source": "wf.head(n = 20)", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "## Stem the Words\n\nYou have cleaned the tweet text and removed stop words. There is one last data preparation step required, stemming the words. Stemming is a process of reducing words to their stems or roots. For example, conjugated verbs such as \"goes\", \"going\", and \"gone\" are stemmed to the word \"go\".  Both Python and R offer a choice of stemmers. Depending on this choice, the results can be more or less suitable for the application. In this case, you will use the popular Porter stemmer. \n\nThe Porter stemmer used by the **PorterStemmer** function in the **nltk.stem.porter** library. Execute the code in the cell below to load and apply the Porter stemmer to the tweet text and display the first few tweets with stemmed words."}, {"source": "from nltk.stem.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\ntemp = [tweet.split() for tweet in tweets] ## Split tweets into tokens\ntemp = map(lambda t: [porter_stemmer.stem(w) for w in t], temp)\ntweets = [' '.join(tweet) for tweet in temp] ## Join the words of the tweet string\ntweets[:10]\n", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "Compare the text in the tweets to the text for the normalized text. Notice that there are fewer words in these tweets following the removal of stop words. Also, words like 'happy' have been stemmed to 'happi'.  This text is now ready for analysis!\n\nTo display the bar plot of the word frequency, execute the code in the cell below."}, {"source": "wf = to_TF(tweets)\nwf_bar(wf)", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "To display the CDF plot of the word frequency, execute the code in the cell below."}, {"source": "plot_cfd(wf)", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "Compare these two charts using the stemmed words to the charts created with just stop word filtering and notice the differences. These differences are quite noticeable. For example, some words like \"good\" and \"like\" have moved higher in the order of most frequent words, while some other words like \"going\" have moved down. "}, {"metadata": {}, "cell_type": "markdown", "source": "****\n\nYou will now examine the head of the resulting word frequency data frame to determine the following:\n\n- Have any of the words in the list been stemmed? \n- Has the stemming changed the frequency of these words?\n\nTo perform this exercise, apply the Pandas **head** method, with **n = 20**, to the wf data frame.\n****"}, {"source": "wf.head(n = 20)", "metadata": {"collapsed": false}, "cell_type": "code", "outputs": [], "execution_count": null}], "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "version": "3.4.5", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}, "nbformat_minor": 0}